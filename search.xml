<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[聊聊面试]]></title>
    <url>%2F2018%2F01%2F13%2F%E8%81%8A%E8%81%8A%E6%8A%80%E6%9C%AF%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[该怎么选择选择比努力重要，这句话在面试以及选择offer中显得很重要，当你结束上一个工作后，一定要想清楚下一份工作要怎么选择，这和职业规划又息息相关了。比如，这次找工作我只认准必须得去成熟大公司去练级，而去年找工作我也是只奔着创业公司去的。人在不同阶段会面对不同的选择，有时候很难一次想清未来几年或者长久自己的方向，尤其对于中国这样从小被父母和老师安排各种事情长大的人，当真正独立的时候，我们几乎完全不知道怎么去做选择。但是想清楚这个问题又很重要，所以我的做法是审视自己的不足，根据自己的不足去迈出下一步。当我想去创业公司的时候我认为我必须有个机会去全局掌控一个项目或者产品，而这个机会只有创业公司能给，当我想去成熟公司的时候我认为我必须要在技术深度和管理上充电，这也是成熟公司才能给的。 被面试这次找工作我只参加了3次面试，美团、京东和滴滴，其实非常想去阿里，但是因为某些原因阿里的面试迟迟没有收到，比较遗憾。好在这三家公司对我来说已经足够有吸引力了。幸运的是，三家都收到了offer，最终选择了滴滴。 准备我算是从上家公司裸辞，实在是因为当时已经无法忍受了。现实证明，裸辞是非常不好的行为，会对你产生很大的压力。办完离职手续，我花了3天时间（周六、日、一）准备面试，对基本能考察的知识点做了一次系统的review，mysql总结系列文章就是在这几天的产物。为什么要做准备？这就跟备考一样，考试考得知识点非常全面，但往往技术人员只能精通一到两项，当然不包括基础。知乎上有个问题：Leetcode233题全部刷完是什么水平，能拿到什么水平的offer？有人回答：全部背完，没有进不了的公司。虽然不一定正确，但是也确实看出面试准备的重要性。这里我只列出我认为5年左右Java工程师准备面试应该准备的技术点：项目介绍（重点是项目架构）、mysql（重点索引、事务、集群高可用）、redis（数据结构、使用场景、集群高可用、缓存相关）、zookeeper（使用场景、paxos算法、分布式锁）、Netty（线程模型、NIO、epoll、零拷贝、踩过的坑）、Dubbo（RPC、源码如远程暴露、服务注册、集群容错等）、Spring（Ioc、DI、AOP等）、Java并发（锁、volatile、ThreadLocal、线程池、并发容器如ConcurrentHashMap等）、Java内存模型、JVM相关、Java类加载机制、分布式微服务（做高可用高并发系统）、线上环境问题排查案例、数据结构和算法（可以刷刷Leetcode）等。当然根据不同职业也有不同的要求，如搜索、大数据、网络通信如TCP/IP等等。 面试流程通过这次找工作的几次面试以及我之前面试别人的经历，我越发感觉面试流程的重要性，当然这是人事需要考虑的事，但是对于候选人来说，可能就决定了offer的选择倾向，毕竟找工作是一个双向选择的过程。技术面试，当然首要目的是考察候选人的技术能力，2-3轮面试非常有必要，并且要由浅入深，根据职位岗位的不同逐渐深入。最后一轮面试我个人认为更重点要考察候选人的性格、是否能很快融入当前团队、之后的定位等，如果觉得合适，还应该向候选人展示当前公司或团队的实力、优势、愿景、展示领导nice的一面，这是吸引候选人很重要的一面。2-3轮技术面试对于技术的考察足矣，剩下一轮（不考虑人事面试）就是双方互相考察的一轮了。 面试到底该考察什么面试流程很重要，面试准备很重要，那么面试的时候到底考察或者被考察什么？我一直认为，最应该关注的是能力，是学习能力、解决问题的能力、团队配合能力、总结能力，而不是这个人掌握了哪个知识点，具备什么样的经验。所以，对面试来说，过程可能比结果更重要，某一个问题可能得不出最终正确的答案，但解决问题的过程更重要。同时，对一个技术人员来说，我一直认为学习能力是技术人最重要的一个能力。对于面试提出的问题，一个很简单判断问题好坏的方式是看是否能很快google或者百度得到答案，比如，StringBuffer和StringBuilder的区别，Linux查看CPU占用的命令等等，这些都不是好问题。这种问题可以得出的结论是候选人掌握了这个知识点，但无法得出候选人具备什么样的能力。所以我更倾向于提一个较综合或者实际开发中碰到的问题案例，考察候选人的思路、经验和过往掌握的能力。 最后，发个广告，滴滴出行，大平台，足够的发展空间，安卓、iOS、Java、产品、测试等等职位，可以联系我内推，chuanqicc0430@gmail.com 或者微信：chuanqicc0430。]]></content>
      <categories>
        <category>技术漫谈</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql总结之集群和高可用]]></title>
    <url>%2F2018%2F01%2F13%2Fmysql%E6%80%BB%E7%BB%93%E4%B9%8B%E9%9B%86%E7%BE%A4%E5%92%8C%E9%AB%98%E5%8F%AF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[mysql集群方案mysql集群原理mysql集群是一个无共享的 （shared-nothing）、分布式节点架构的存储方案，其目的是提供集群容错性（高可用）和高性能。数据在单个数据节点（也叫做存储节点） 上存储和复制，每个数据节点运行在独立的服务器上并维护数据的一份拷贝。在集群中还有管理节点，通过管理节点维持集群的运行，保证集群的整体可用性。数据更新时使用读取已提交隔离级别（read-commited isolation），来保证所有节点数据的一致性；使用两阶段提交机制（tow-phased commit）保证所有节点都有相同的数据（任何一个写操作失败，则更新失败）。无共享的对等节点使得某个服务器上的更新操作在其他服务器上立即可见。传播更新使用一种复杂的通信机制，这一机制专门用来提供跨网络的高吞吐量。该架构通过多个mysql服务器分配负载，从而最大程度的达到高性能，通过在不同的位置存储数据来保证高可用和冗余。mysql集群的典型部署是在某个网络中的不同机器上面安装集群的各个组建，因此mysql集群有称之为 网络数据库（network database 或者 NDB）。 常见mysql集群方案]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql总结之Innodb事务和锁]]></title>
    <url>%2F2018%2F01%2F12%2Fmysql%E6%80%BB%E7%BB%93%E4%B9%8BInnodb%E4%BA%8B%E5%8A%A1%E5%92%8C%E9%94%81%2F</url>
    <content type="text"><![CDATA[事务事务（Transaction）及其ACID属性事务是由一组SQL语句组成的逻辑处理单元，事务具有4个属性：原子性（Atomicity）、一致性（Consistent）、隔离性（Isolation）、持久性（Durable），通常简称为事务的ACID属性。 并发事务处理带来的问题相对于串行处理来说，并发事务处理能大大增加数据库资源的利用率，提高数据库系统的事务吞吐量，从而可以支持更多的用户。但并发事务处理也会带来一些问题，主要包括以下几种情况。 - 更新丢失（Lost Update）：当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题－－最后的更新覆盖了由其他事务所做的更新。 - 脏读（Dirty Reads）：一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象地叫做&quot;脏读&quot;。 - 不可重复读（Non-Repeatable Reads）：一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读”。 - 幻读（Phantom Reads）：一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。 事务隔离级别在上面讲到的并发事务处理带来的问题中，“更新丢失”通常是应该完全避免的。但防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决，因此，防止更新丢失应该是应用的责任。“脏读”、“不可重复读”和“幻读”，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。数据库实现事务隔离的方式，基本上可分为以下两种。 一种是在读取数据前，对其加锁，阻止其他事务对数据进行修改。 另一种是不用加任何锁，通过一定机制生成一个数据请求时间点的一致性数据快照（Snapshot)，并用这个快照来提供一定级别（语句级或事务级）的一致性读取，也叫做数据多版本并发控制（MultiVersion Concurrency Control，简称MVCC或MCC）。 数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上 “串行化”进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对“不可重复读”和“幻读”并不敏感，可能更关心数据并发访问的能力。4个事务隔离级别如下表所示，每个级别的隔离程度不同，允许出现的副作用也不同。 隔离级别 读数据一致性 脏读 不可重复度 幻读 未提交读（Read uncommitted） 最低级别，只能保证不读取物理上损坏的数据 是 是 是 已提交度（Read committed） 语句级 否 是 是 可重复读（Repeatable read） 事务级 否 否 是 可序列化（Serializable） 最高级别，事务级 否 否 否 大多数的数据库系统的默认事务隔离级别都是：Read committed，而MySQL的默认事务隔离级别是：Repeatable Read，修改mysql事务隔离级别可以通过下列语句：修改事务权限的语句是：set [ global | session ] transaction isolation level Read uncommitted | Read committed | Repeatable | Serializable; Innodb锁InnoDB的行锁模式及加锁方法InnoDB实现了以下两种类型的行锁。 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 排他锁（X)：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。 另外innodb内部还有两种表锁：意向共享锁（IS）和意向排他锁（IX），这两种锁是InnoDB自动加的，不需用户干预。对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)；对于普通SELECT语句，InnoDB不会加任何锁；事务可以通过以下语句显示给记录集加共享锁或排他锁。 共享锁（S）：SELECT * FROM table_name WHERE … LOCK IN SHARE MODE。 排他锁（X)：SELECT * FROM table_name WHERE … FOR UPDATE。 InnoDB行锁实现方式InnoDB行锁是通过给索引上的索引项加锁来实现的，这意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。在实际应用中，要特别注意InnoDB行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。不论是使用主键索引、唯一索引或普通索引，InnoDB都会使用行锁来对数据加锁。另外，即便在条件中使用了索引字段，但是否使用索引来检索数据是由MySQL通过判断不同执行计划的代价来决定的，如果MySQL认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁。因此，在分析锁冲突时，别忘了检查SQL的执行计划，以确认是否真正使用了索引 Next-Key锁当用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）。举例来说，假如emp表中只有101条记录，其empid的值分别是 1,2,…,100,101，下面的SQL：Select * from emp where empid &gt; 100 for update;是一个范围条件的检索，InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁。很显然，在使用范围条件检索并锁定记录时，InnoDB这种加锁机制会阻塞符合条件范围内键值的并发插入，这往往会造成严重的锁等待。因此，在实际应用开发中，尤其是并发插入比较多的应用，要尽量优化业务逻辑，尽量使用相等条件来访问更新数据，避免使用范围条件。 表锁 事务需要更新大部分或全部数据，表又比较大，如果使用默认的行锁，不仅这个事务执行效率低，而且可能造成其他事务长时间锁等待和锁冲突； 事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚。 以上两种情况可以考虑显式使用表锁：LOCK TABLE，注意使用LOCK TABLES要将AUTOCOMMIT设为0，否则MySQL不会给表加锁。 死锁Innodb可能会发生死锁，比如两个事务都需要获得对方持有的排他锁才能继续完成事务，这种循环锁等待就是典型的死锁。发生死锁后，InnoDB一般都能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务。但在涉及外部锁，或涉及表锁的情况下，InnoDB并不能完全自动检测到死锁，这需要通过设置锁等待超时参数 innodb_lock_wait_timeout来解决。需要说明的是，这个参数并不是只用来解决死锁问题，在并发访问比较高的情况下，如果大量事务因无法立即获得所需的锁而挂起，会造成严重性能问题，甚至拖跨数据库。通过设置合适的锁等待超时阈值，可以避免这种情况发生。通常来说，死锁都是应用设计的问题，通过调整业务流程、数据库对象设计、事务大小，以及访问数据库的SQL语句，绝大部分死锁都可以避免。避免死锁的常用方法如下： 如果不同的应用并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会； 以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低出现死锁的可能。 在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应先申请共享锁，更新时再申请排他锁，因为当用户申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁； 在REPEATABLE-READ隔离级别下，如果两个线程同时对相同条件记录用SELECT…FOR UPDATE加排他锁，在没有符合该条件记录情况下，两个线程都会加锁成功。程序发现记录尚不存在，就试图插入一条新记录，如果两个线程都这么做，就会出现死锁。这种情况下，将隔离级别改成READ COMMITTED，就可避免问题。 当隔离级别为READ COMMITTED时，如果两个线程都先执行SELECT…FOR UPDATE，判断是否存在符合条件的记录，如果没有，就插入记录。此时，只有一个线程能插入成功，另一个线程会出现锁等待，当第1个线程提交后，第2个线程会因主键重出错，但虽然这个线程出错了，却会获得一个排他锁，这时如果有第3个线程又来申请排他锁，也会出现死锁。对于这种情况，可以直接做插入操作，然后再捕获主键重异常，或者在遇到主键重错误时，总是执行ROLLBACK释放获得的排他锁 如果出现死锁，可以用SHOW INNODB STATUS命令来确定最后一个死锁产生的原因。返回结果中包括死锁相关事务的详细信息，如引发死锁的SQL语句，事务已经获得的锁，正在等待什么锁，以及被回滚的事务等。如下示例，据此可以分析死锁产生的原因和改进措施。 12345678910111213141516171819202122232425mysql&gt; show innodb status…….------------------------LATEST DETECTED DEADLOCK------------------------070710 14:05:16*** (1) TRANSACTION:TRANSACTION 0 117470078, ACTIVE 117 sec, process no 1468, OS thread id 1197328736 insertingmysql tables in use 1, locked 1LOCK WAIT 5 lock struct(s), heap size 1216MySQL thread id 7521657, query id 673468054 localhost root updateinsert into country (country_id,country) values(110,'Test')………*** (2) TRANSACTION:TRANSACTION 0 117470079, ACTIVE 39 sec, process no 1468, OS thread id 1164048736 starting index read, thread declared inside InnoDB 500mysql tables in use 1, locked 14 lock struct(s), heap size 1216, undo log entries 1MySQL thread id 7521664, query id 673468058 localhost root statisticsselect first_name,last_name from actor where actor_id = 1 for update*** (2) HOLDS THE LOCK(S):………*** (2) WAITING FOR THIS LOCK TO BE GRANTED:………*** WE ROLL BACK TRANSACTION (1)……]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql总结之Innodb索引]]></title>
    <url>%2F2017%2F12%2F31%2Fmysql%E6%80%BB%E7%BB%93%E4%B9%8BInnodb%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[在上一篇mysql总结之Innodb引擎中，提到过，Innodb索引是聚集索引，索引文件即是数据文件，每个表都有一个主键，即使用户没有显式的设置一个主键，mysql也会为这张表生成一个主键。那么，Innodb索引文件的数据结构是什么？数据查询时怎么通过索引查询的呢？ Innodb索引Innodb索引原理InnoDB使用的是聚集索引，将主键组织到一棵B+树中，而行数据就储存在叶子节点上。B+树是指多路平衡搜索树，一棵典型的B+树结构如下图关于B+树和Innodb具体怎么构建实现B+树，在这里不详细说，可参考下面两个blog，有非常详细的介绍：http://www.cnblogs.com/tgycoder/p/5077017.htmlhttp://blog.csdn.net/voidccc/article/details/40077329需要记住： B+树的非叶子节点即是索引节点，存的是顺序的主键索引区间。 所有数据都是按键值的大小顺序存放在同一层的叶子节点中，各叶子节点用指针进行连接 实际数据库B+树的高度一般在2-3，因此只需2-3次磁盘I/O就可以定位到需要的数据页，而实际情况中，B+树的非叶子节点一般直接预加载内存中，这样检索速度会更快。 主键索引查询通过主键检索数据就是一次B+树的遍历过程，时间复杂度为O(h)，h为B+树的高度。需要注意的地方是，B+树索引不能找到具体的一条记录，而是只能找到对应的数据页。把数据页从磁盘装入到内存中，再通过Page Directory（page的内部结构之一）进行二分查找，二分查找如果能找到行记录，则直接返回，如果找不到，还需要从链表中查找。 辅助索引查询辅助索引会单独构建一棵辅助索引B+树，与主键索引树不同的是，辅助索引树的叶子节点不存行的全部数据，而是只存键值和对应的主键值，因此，辅助索引树的查询需要经过两次树的检索，举例，如下图表包含三个字段，id为主键，Name为辅助索引：若对Name列进行条件搜索，则需要两个步骤：第一步在辅助索引B+树中检索Name，到达其叶子节点获取对应的主键。第二步使用主键在主索引B+树种再执行一次B+树检索操作，最终到达叶子节点即可获取整行数据。检索如下图所示： 联合索引查询联合索引还是一个B+树，不同的是联合索引键值的数量不是1，而是大于等于2。假设两个字段a和b组成一个联合索引，如下图所示，每个节点上有两个键值，(1,1),(1,2),(2,1),(2,4),(3,1),(3,2), 数据按(a,b)顺序进行排列。因此，对于查询SELECT * FROM t WHERE a=1 AND b=2,显然可以使用(a,b)这个联合索引。对于单个a列查询SELECT * FROM t WHERE a=1也是可以使用(a,b)这个索引。但是对于b列的查询SELECT * FROM t WHERE b=2是用不到这颗B+树索引。可以看到叶节点上b的值为1、2、1、4、1、2.显然不是排序的，因此b列的查询使用不到(a,b)索引。这是一个比较重要的原则，即索引的最左前缀匹配原则，这在后面的索引使用和优化中会经常提到。 B+树索引的优势为什么不用二叉查找树比如红黑树构建索引树？为什么使用B+树而不是B树？首先，评价一个数据结构作为索引的数据结构最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的数据结构要尽量减少查找过程中磁盘I/O的存取次数。使用二叉查找树，比如红黑树构建索引树，树的高度会非常深，由于逻辑上很近的节点（父子）物理上可能很远，无法利用磁盘预读原理，会大大增加磁盘I/O次数。那么为什么不用B树？ B+树的磁盘读写代价更低B+树的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B 树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。举个例子，假设磁盘中的一个盘块容纳16bytes，而一个关键字2bytes，一个关键字具体信息指针2bytes。一棵9阶B-tree(一个结点最多8个关键字)的内部结点需要2个盘快。而B+树内部结点只需要1个盘快(全部关键字都在叶结点的缘故？)。当需要把内部结点读入内存中的时候，B-树就比B+树多一次盘块查找时间(在磁盘中就是盘片旋转的时间)(B+树的内结点只有索引的作用，何来“把内部结点读入内存”…,对于B+树找到叶结点就可以，另外B+树可以顺序查找)。 B+树的查询效率更加稳定由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 Innodb索引使用和优化经验索引建立原则在合理的表结构设计基础上的数据查询优化，很大程度上由索引的合理使用决定，因此，索引的正确使用非常重要，索引的建立应遵循以下原则： 尽量使用主键索引，每个表都应该有一个业务无关的自增主键，在主键索引上的排序查找和范围查找的速度非常快。 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整； =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式； 尽量选择区分度高的列作为索引,区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0； 索引字段应尽量小，不推荐在超大varchar或text字段上加索引; 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可 索引并非是越多越好，尤其对于写操作较为频繁的业务，索引的维护成本会比较高。 为什么要尽量使用一个业务无关的自增字段作为主键？Innodb索引是聚集索引，数据记录按主键顺序存在主索引树的叶子节点，如果使用自增主键，那么每次插入新纪录，就会顺序添加到当前索引节点的后续位置，当一页写满，则开辟新的一页，每次插入不需要移动数据，减少维护索引的成本。如果使用非自增主键，插入主键的值接近随机，因此每次插入都需要移动数据，索引维护成本大，且频繁的移动可能会造成大量的磁盘碎片。 SQL优化这里的SQL优化是指对单条SQL的优化。 使用explian分析执行计划，我的习惯是，写代码的时候，对于稍微复杂的SQL一定要用explain看一下，分析索引是否用到，是否有优化空间。explian执行计划主要关注这几项： type : 查询access的方式，表的连接类型index | 索引 full | 全表扫描 ref | 参照查询，也就是等值查询 range | 范围查询 key:本次查询最终选择使用哪个索引，NULL为未使用索引 key_len:选择的索引使用的前缀长度或者整个长度 rows:查询逻辑扫描过的记录行数 extra:包含不适合在其他列中显示但十分重要的额外信息：Using index | 表示相应的select操作中使用了覆盖索引（Covering Index）【注：MySQL可以利用索引返回select列表中的字段，而不必根据索引再次读取数据文件 包含所有满足查询需要的数据的索引称为 覆盖索引】 Using where | 表示MySQL在存储引擎受到记录后进行“后过滤”（Post-filter）,如果查询未能使用索引，Using where的作用只是提醒我们MySQL将用where子句来过滤结果集 Using temporary | 表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询 Using filesort | MySQL中无法利用索引完成的排序操作称为“文件排序” inner join代left join或right join，inner join性能比较快，等值连接隐式的使用inner join，如 SELECT A.id,A.name,B.id,B.name FROM A,B WHERE A.id = B.id; 子查询的性能比join性能慢，尽量用外连接来替换子查询。 join查询应用小表驱动大表，即left join左边的表结果应尽量小，right join相反。 索引列不能参与计算，不能用于函数计算，比如from_unixtime(create_time) = ‘2017-12-31’就不能使用到索引，原因很简单，B+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该优化成create_time = unix_timestamp(‘2017-12-31’); 联合索引，只要有一列含有NULL值，这一列就不会用到索引，因此创建表时，字段不应默认为NULL。 join查询，表关联的字段应是相同数据类型，否则用不到索引。 order by的字段也应该满足最左前缀匹配原则，否则不会用到索引，例如a和b字段组成联合索引，以下order by语句可以用到索引： order by a; a = 3 order by b; order by a,b; order by a desc ,b desc; a &gt; 5 order by a; %通配符的使用，like ‘%xxx’或like ‘%xxx%’都用不到索引。 尽量使用union all或者union代替OR，尽量使用union all而不是union。 not in和&lt;&gt;都不会用到索引，not in可以用not exists代替，&lt;&gt;可以用or代替。 limit查询在数据量很大时会有性能问题，如select * from t order by id limit 100000,10;可以优化成：select * from t where id in(select id from t limit 100000,1) limit 10;或者select * from t where id between 100000 and 100010;后者性能最好。 用limit 1取得唯一行，有时要查询一张表时，如果只需要检索一行，可以使用limit 1来终止数据库引擎继续扫描整个表或者索引。 避免使用select *，会增加磁盘操作时间，增加网络延迟，浪费流量。 避免索引字段的值，因为索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。 大数量下的数据查询优化方案上一节的SQL优化针对的是具体的单条的SQL优化经验，对于大数据量下的数据查询优化，我们更应该从全局角度去思考这个问题，总结一下： 表结构的适度冗余，合理设计表，适当的反第三范式，合理冗余字段，可以极大简化数据模型。 优化SQL和索引； 加缓存，如redis或memcached； 表的垂直拆分，大系统拆成小系统，数据分离，使数据和查询分散； 水平拆分，如分库分表设计； 上面从简单到复杂，对应的实施成本也不断上升，因此，这个要跟随整个架构升级而进行。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql总结之Innodb引擎]]></title>
    <url>%2F2017%2F12%2F30%2Fmysql%E6%80%BB%E7%BB%93%E4%B9%8BInnodb%E5%BC%95%E6%93%8E%2F</url>
    <content type="text"><![CDATA[mysql是Java工程师面试时最经常被问到的技术点，因为mysql技术点太多了，很容易考察出被面试人的技术广度和深度。现总结一下mysql的基础知识点，但只是一份入门级的总结，不会太深入，期望能尽量覆盖到mysql常用知识点。另外，因为是总结，有些知识点会直接从其他地方拷贝，包括图片（图侵删）等，不喜勿喷。 本篇总结主要按照以下结构分别阐述： mysql数据引擎，重点介绍innodb数据引擎 innodb与myisam引擎的区别 innodb的逻辑存储结构 innodb 特性 innodb索引 innodb索引原理 索引的使用和优化经验。 SQL优化经验和大数据量下的数据存储优化 mysql事务和锁 mysql集群和高可用 mysql主从同步原理 mysql集群搭建方案 mysql高可用方案 mysql数据引擎，innodb引擎mysql数据引擎主要有MyIsam、Innodb、Merge、Blackhole、Berkeley、CSV、Cluster/NDB等引擎。用的最多的就是MyIsam和Innodb两种引擎。 MyIsam引擎和Innodb的区别 Innodb提供了对数据库ACID事务的支持，MyIsam不支持事务； Innodb支持行级锁，MyIsam只支持表级锁； Innodb支持外键约束，MyIsam不支持； Innodb索引是聚集索引，索引文件即是数据文件，后面说到索引的时候会详细说明，MyIsam是非聚集索引，索引文件不存实际数据。 MyIsam支持全文索引，并且存储了表的行数，Innodb不支持全文索引，没有存储表的行数，因此 SELECT COUNT(*) FROM TABLE 会引起全表扫描。 Innodb的逻辑存储结构主要介绍InnoDB存储引擎表的逻辑存储以及实现。 索引组织表先看一个官方文档说明： clustered indexThe InnoDB term for a primary key index. InnoDB table storage is organized based on the values of the primary key columns,to speed up queries and sorts involving the primary key columns. For best performance, choose the primary key columns carefullybased on the most performance-critical queries. Because modifying the columns of the clustered index is an expensive operation,choose primary columns that are rarely or never updated.In the Oracle Database product, this type of table is known as an index-organized table. 翻译一下就是，Innodb数据存储是基于主键值的，通过主键可以提高查询和排序速度。因此，为了提高性能，应该谨慎的选择主键字段。修改聚集索引的字段是一个昂贵的操作，主键不应该被修改。在Oracle的数据库产品中，这种表叫做索引组织表。 按照主键的顺序存储，即索引组织表，innodb中的每个表都有一个主键，即使用户没有显式的设置一个主键，mysql也会为这张表生成一个主键，主键的生成规则： 用户显式的创建主键字段； 如果显式创建主键字段，判断表中是否有非空的唯一索引，若有，则为主键； 如果1，2都不满足，则Innodb自动生成一个6字节大小的隐式主键。 索引组织表Innodb中所有的数据都被逻辑地存放在一个空间中，称之为表空间(tablespace)。表空间又由段(segment)、区(extent)、页(page)、行（row）组成。InnoDB存储引擎的逻辑存储结构如下图表空间 Tablespace表空间是InnoDB存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。表空间又分为独立表空间和共享表空间。通过参数innodb_file_per_table参数来决定使用何种类型的表空间。但是需要注意的是独立表空间内只存放数据、索引和插入缓冲页，其他的数据，如回滚(undo)信息、插入缓冲索引页、系统事务信息、二次写缓冲(double write buffer)等还是放在共享表空间中。段 Segment表空间由各个段组成。常见的段有数据段、索引段、回滚段等。InnoDB存储引擎是索引组织表，因此数据及索引，索引即数据。数据段即为B+树的叶子点(leaf node segment)，索引段为B+数据的非叶子节点(non-leaf node segment)。区 Extent区是由连续页组成的空间。InnoDB存储引擎页的大小为16KB，一个区有64个连续的页组成，所以每个区的大小都是1MB。InnoDB 1.0.x版本开始引入压缩页，即每个页的大小可以在建表时通过参数key_block_size设置为2K、4K、8K，因此每个区对于页的数量就为512、256、126。InnoDB 1.2.X版本新增参数innodb_page_size将默认页的大小设置为4K、8K，但是页中的数据不是压缩，这是其中的数量同样为256、128。一句话，不论页的大小怎么变化，区的大小不变1M。但是有这样一个问题：在开启独立表空间之后，创建的表默认大小是96K，区中是64个连续的页，创建的表空间应该是1M才对呀？这是因为在每个段的开始时，先用32个页大小的碎片页(fragment page)来保存数据，在使用完这些页之后才是64个连续的页的申请。这样做是对于一些小表或者undo这类的段，可以在开始时申请较少的空间，节省磁盘容量的开销。页 Page页是InnoDB磁盘管理的最小单位。默认大小为16K，可以通过innodb_page_size将页的大小设置为4K、8K、16K，则所有表中页的大小都为设置值，不可以对其再次修改。除非通过mysqldump导入和导出操作来产生新的库。常见的页的类型有:数据页(B-tree Node)、undo页(unod Log Page)、系统页(System Page)、事务数据页(Transaction system Page)、插入缓冲空闲列表页(Insert Buffer Free List)、未压缩的二进制大对象页(Uncompressed BLOB Page)、压缩的二进制对象页(compressed BLOB Page)。行 RowInnoDB存储引擎是面向行的(row-oriented)，也就是说数据是按行进行存放的。每个页存放的行记录也是有硬性定义的，最多运行存放16K/2-200行的记录，即7992行记录。 Innodb关键特性innodb存储引擎的关键特性包括：插入缓冲，两次写，自适应哈希索引插入缓冲主键是行唯一的标识符，在应用程序中行记录的插入顺序是按照主键递增的顺序进行插入的，因此，插入聚集索引一般是顺序的，不需要磁盘的随机读取。Innodb设计了插入缓冲，对于非聚集索引的插入或更新插入，不是每一次直接插入索引页，而是先判断插入的非聚集索引页是否在缓冲池。如果在，则直接插入；如果不在，则先放入一个插入缓冲区中，然后再以一定的频率执行插入缓冲和非聚集索引页子节点的合并操作，这时通常能将多个插入合并到一个操作中，这就大大提高了对非聚集索引执行插入和修改操作的性能。插入缓冲的两个条件：索引是辅助索引；索引不是唯一的。 两次写当数据库死机时，可能发生数据库正在写一个页面，而这个页只写来了一部分，我们称之为部分写失效。有人想如果发生写失效，可以重做日志（redo log）进行恢复。这是一个办法的但必须清楚，重做日志分析记录的是对页的物理操作，如果这个页本身已经损坏（脏页），再对其重做也没意义。当写入部分失效时，先通过页的副本来还原该页，再进行重做，这就是double write。double write由两部分组成：一部分是内存池中的double write buffer，另一部分是物理磁盘上的共享表空间中的连续的128个页，即两个区。当缓冲页的脏页刷新时，并不直接写磁盘，而是会通过memcpy函数将脏页先拷贝到内存中的double write buffer .之后通过double write buffer再分两次，每次写入1MB到共享表空间的物理磁盘上，然后马上调用fsync函数，同步磁盘，避免缓冲带来的问题。 自适应哈希引擎哈希是一种非常快的查找方法，一般情况下查找时间的复杂度为O（1），常用于连接操作，Innodb会监控对表上索引的查找，如果观察到建立哈希索引可以带来速度的提升，则自动建立哈希索引，所以称之为自适应的。自适应哈希索引通过缓冲池中的B++树构造而来，因此建立的速度会很快。而且不需要将整个表都建立哈希索引，Innodb会自动根据访问的频率和模式来为某些页建立哈希索引。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017年总结]]></title>
    <url>%2F2017%2F12%2F30%2F2017%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[生活儿子的出生当是2017年的头等大事，我自己非常喜欢小孩，对于儿子的出生自然喜不自胜。儿子出生那天，我说希望儿子以后健康、独立、快乐，这也是我对儿子未来的祝福。健康要放在首位，身体好才能担得起未来你身上的责任。独立，是希望你以后要学会思考，要有自己的个性，不盲目从众，以独立之思考、决定从容应对以后的工作和生活。快乐，是对你人生的祝福，希望你能感受到这个世界的美好，有一个美好的人生，也希望你以后莫要陷入人生的陷阱，人生可以很难，也可以很简单，希望你无时无刻都快乐。 老婆生孩子糟了不少罪，很是心疼，我本不是一个浪漫的人，不会制造惊喜，日子过得波澜不惊，幸而老婆也不是很在意。新的一年，一定要多多疼爱老婆，逢年过节，一束鲜花，以表心意。 父母家人都十分健康，幸甚，又添一侄子和外甥，家族日益庞大，父母老了，还要帮忙照看孩子，也是辛苦，然总也是老来之乐。 老婆的爷爷今年去世，让我想起了我爷爷去世，爷爷去世没来的及回去见最后一面，是我心头一大憾事，世事难料，对老人需要倍加珍惜。 2018年要保持身体健康，17年阑尾炎发作以及后面的手术，糟了不少罪。跑步要坚持下去，另外需要给家人买一份合适的保险。 其他没什么值得书写，如房子至今仍未装修，买车等等，一笔带过。 工作工作上，未来虽然仍不是很通透，但今年对我来说是非常重要的一年，想明白了一些事，也到了该想清楚以后的规划的时候了，希望2018年是爆发的一年。 2017年，一整年都还是在这家创业公司，最近刚刚办理了离职，从2016年4月份加入，1年8个月的时间，不长也不短。这段时间主要做了这么几件事： 从头开始搭建服务端架构，以dubbo为基础框架，结合自研的一些组件，完成了这套服务器架构，现在看，有很多不足，但足够公司用一两年，有这么长时间，架构升级就水到渠成了。 从16年12月份到17年4月份的架构重构项目，这是我带的最大的团队和项目，包括公司产品、Android、iOS和服务器、测试团队的大部分人员，作为项目负责人，带领小伙伴完成了产品梳理、架构设计、研发一直到上线，这个项目给公司以后的发展打下了基础，可以肯定的是，至少几年内，公司业务都还是要以这个架构为基础。 17年下半年，逐步的聚焦服务端团队，进行服务端架构升级和优化，包括服务性能优化、推动服务化、自动化等。 我很感激这段经历，对我自己是一个很好的成长。16年加入的时候，服务器端只有我自己一个人，产品也从头开始梳理，一步步看着服务器端架构搭建完善起来，一步步看着产品成形上线，投入的感情岂止一点两点，然而非常可惜，到最后还是毅然离开了。 回顾这接近两年的创业经历，诸多收获，也有诸多的困惑。当产品一次次推倒重来，当老板一次又一次任性的去设计产品，当僵硬的管理体制一次次限制了开放创新，我只感到力不从心，也害怕这样的工作对我以后会产生怎样的影响，First you hate ‘em, then you get used to ‘em. Enough time passes, gets so you depend on them. That’s institutionalized. –这些墙很有趣。刚入狱的时候，你痛恨周围的高墙；慢慢地，你习惯了生活在其中；最终你会发现自己不得不依靠它而生存。这就叫体制化。《肖申克的救赎》这句台词代表了我当时离职前的心情，也是我离职的最大原因。 然而回过头来看一看，创业哪有那么容易，也许就需要这么多一次次尝试，一次次挖坑埋坑，才能争取一次成功的机会，所以要么做好接受这一切的准备，要么不要去这种创业公司。有同事说，公司正要看到希望，如果真的成功了，你会不会后悔？我想说，怎么叫后悔呢？这个公司成功了就会后悔，没成功就不会后悔么？既然做出了选择，就没有后悔这一说了，所有的事都是你这次选择需要承担的，没有后悔，没有反悔。 那这一年到底想清楚了什么？我以前说过，我的梦想是创业，这一年让我看清了，想创业，实在是还差十万八千里，最重要的是思维，其次基本功还差太多，这个基本功包括很多，技术、管理、人际等等。因此2018年，希望能尽可能的去弥补。简单来说，2018年希望做好这几件事： 技术上要不断提高，开阔技术广度； 管理上不断学习，可能会去学一下管理方面的书籍或者课程，考一个管理方面的证书； 不断认识牛人，怎么认识？一个是公司内的大神，多接触一些，另一个是要多去参加技术论坛讲座，如果自己也能上去讲一下，最好不过了。 维护好自己的技术博客，维护一个技术公众号 2018年对我自己是非常重要的一年，我自己又有诸多懒散的毛病，因此flag立下了，仍然需要下些精力去执行，否则，待明年写2018年总结时又难免一阵苦恼懊悔，非善事也。]]></content>
      <categories>
        <category>生活感悟</category>
      </categories>
      <tags>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo源码分析]]></title>
    <url>%2F2017%2F06%2F20%2FDubbox%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[微服务架构微服务架构相比传统的单体架构，服务之前的依赖关系更加明确，服务职责单一，服务可以自治，在良好的服务治理体系下，可以极大提高系统的容错率以及系统的弹性。 但是相对的，微服务架构由于架构复杂，也会增加很多部署和运维成本，故障排查较困难，链路调用风险增大，团队协作成本也会上升。 如果纯粹是一名技术人员，肯定首选微服务架构，但是，从架构角度考虑，架构不单单是技术的选型，技术团队的方方面面都是需要衡量的，如团队成员的技术水平，开发流程（瀑布或敏捷），项目的大小、团队成员合理的协作方式、开发效率提升等等。其实，个人认为这些才是最难得。 在项目初期，尤其是对于创业公司来说，产品开发初期侧重点势必在于产品需求的实现，但是是基础架构的搭建不是一朝一夕就能完善的，这点要做好准备，基础平台的建设开始的越早越好，当然，前提是，老板支持。 DubboxDubbox(https://github.com/dangdangdotcom/dubbox)是当当网在阿里开源的Dubbo(http://dubbo.io/)基础上扩展而来。Dubbo阿里已经不维护了，Dubbox目前当当网基本也不再维护了，但是国内的Dubbo和Dubbox的用户还是非常多的，虽然社区活跃程度较低，但是基本问题都会找到解决方案的。建议有兴趣的同学研究一下Spring Cloud，再做决定。 严格来说，Dubbo或者Dubbox不是微服务架构的全套解决方案，个人认为Dubbox只是提供了服务发现、服务治理、服务降级的RPC框架。其他的组件需要自己开发或者引入第三方开源框架，如配置管理，DB Proxy，日志以及链路调用分析，熔断机制等等。有兴趣的同学，可以看下本人开源的Linkz(https://github.com/chuanqicc0430/Linkz)，提供了几个组件，可以结合Dubbox使用。 Dubbo主要提供服务注册发现、负载均衡、服务编排、服务降级、底层通信（可选Netty或Mina）、序列化等功能，架构图如下： 接下来，我会将阅读Dubbox的源码写下来，有兴趣的同学可以一块学习。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>Dubbo</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Netty实现的httpserver实现二]]></title>
    <url>%2F2016%2F02%2F19%2FNetty-Based-HttpServer-2%2F</url>
    <content type="text"><![CDATA[代码结构代码结构如下图所示： 其中： app: HttpApplication是业务层的基类，继承此类以实现具体业务；HttpApplication可携带上下文ApplicationContext，ApplicationContext提供编解码方法。 core: 核心层，配置、日志、过滤器、拦截器等都在这一层实现。 appHttpApplication12345678910111213public abstract class HttpApplication&lt;C extends ApplicationContext&gt; &#123; protected HttpApplication() &#123; &#125; public abstract void load() throws Exception; public abstract void unload() throws Exception; public abstract void process(ApplicationTx&lt;C&gt; tx) throws Exception;&#125; load(): 初始化资源；unload(): 释放资源；process(ApplicationTx tx): 执行业务，tx携带用户上下文和本次请求的httprequest和httpresponse； HttpPrefix12345678910@Retention(RetentionPolicy.RUNTIME)@Target(&#123; ElementType.TYPE &#125;)public @interface HttpPrefix &#123; String path(); String protocal(); HttpMethod[] method() default HttpMethod.GET;&#125; HttpPrefix注解标注了HttpApplication的请求路径，协议和方法（Post or Get）。 ApplicationContext123456789101112131415161718192021public abstract class ApplicationContext &#123; /** * * 从数据中解码 * * @param datas * @throws IOException */ public abstract void decode(byte[] datas) throws Exception; /** * * 按需编码 * * @param demand * @return */ public abstract byte[] encode(int demands) throws IOException;&#125; 用户上下文context基类，可自定义业务需要的contenxt类型。 ApplicationTx123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class ApplicationTx&lt;C extends ApplicationContext&gt; &#123; private static final Logger LOGGER = LoggerFactory.getLogger(ApplicationTx.class); private C context = null; private byte[] contextData; private HttpServletRequest request; private HttpServletResponse response; public ApplicationTx(HttpServletRequest request, HttpServletResponse response) &#123; this.request = request; this.response = response; contextData = base64decode(request.getHeader("ContextData")); &#125; public C context() &#123; return context; &#125; public void setContext(C context) &#123; this.context = context; &#125; /** * &#123;在这里补充功能说明&#125; * * @param header * @return */ private byte[] base64decode(String header) &#123; return Base64.decode(header); // 暂不用decodeFast &#125; public HttpServletRequest getRequest() &#123; return request; &#125; public HttpServletResponse getResponse() &#123; return response; &#125; /** * &#123;在这里补充功能说明&#125; * * @return */ protected byte[] extractContextData() &#123; return contextData; &#125; public void processSucceed(String responseStr,String contentType) &#123; response.setStatus(200); response.setHeader("Content-Type", contentType); response.setHeader("Content-Length", String.valueOf(responseStr.length())); response.setHeader("Content-Language", "en"); try &#123; response.getWriter().write(responseStr); &#125; catch (IOException e) &#123; LOGGER.error("Send success response error!", e); &#125; &#125; public void processFailed(CUException error) &#123; response.setStatus(error.getReturnCode()); response.setHeader("Content-Type", "text/plain"); response.setHeader("Content-Length", String.valueOf(error.getMessage().length())); response.setHeader("Content-Language", "en"); try &#123; response.getWriter().write(error.getMessage()); &#125; catch (IOException e) &#123; LOGGER.error("Send failed response error!", e); &#125; &#125;&#125; ApplicationTx保存上下文ApplicationContext和本次请求的httprequest和httpresponse，并提供业务执行成功和失败应答逻辑。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>httpserver</tag>
        <tag>netty</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Netty实现的httpserver实现一]]></title>
    <url>%2F2016%2F02%2F19%2FNetty-Based-HttpServer-1%2F</url>
    <content type="text"><![CDATA[目前http协议是许多App和服务端通信的首选协议，那么对于服务端来说，一个好的httpserver一定是必不可少的了。开源的嵌入式httpServer有很多，就连JDK都自带一个轻量的httpserver，我们项目最初选择的就是JDK自带的httpserver。 从代码结构来看JDK自带的httpserver真的是非常轻量，但是好处越大却缺点也越大，随着系统复杂度的提升，这种轻量的组件越来越力不从心，对开发人员相当不友好，业务与底层耦合太强，代码复杂度越来越高的同时，性能越来越差。 最近生产环境这个server屡次出问题，已经到了没法忍受的地步，而且相对其他httpserver来说，性能并没有很高，因此改造httpserver提升日程。 接下来几篇将介绍本次项目实现的基于netty的高性能httpserver，特点如下： 基于Netty的高性能、高可靠性的特性，可以满足大多数业务场景的并发需求； 业务与底层解耦，开发人员只需关心业务细节即可； 极简的使用方式，注解式编程。 当然本人水平有限，这个组件还有很多不足之处，期待以后有机会完善。 GitHub项目地址：https://github.com/chuanqicc0430/netty-http-container]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>httpserver</tag>
        <tag>netty</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql-多表查询优化]]></title>
    <url>%2F2016%2F02%2F16%2FMysql-%E5%A4%9A%E8%A1%A8%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[猴年开年第一篇，mark一下。 关于mysql的多表查询优化，之前一直没有注意，最近写的一个自认为还OK的sql在生产环境中出现了问题，在此记录一下。 排查机器的CPU占用100%，查看业务日志，在执行问题sql的时候抛出异常：1java.sql.SQLException: An attempt by a client to checkout a Connection has timed out. 这个库连接肯定没问题，因为别的业务也在用这个库，所以定位这条sql写的有问题。问题原因暂且放在一边，先把业务恢复了。1show processlist; 列出所有正在运行的线程信息，发现这条问题sql卡在了 Sending data 状态： 直接执行 kill 线程号，杀掉这个线程，CPU立马就降下来了。1kill 256397; 值得注意的是，Sending data 状态官方解释是:正在处理Select查询的记录，同时正在把结果发送给客户端。如果卡在这个状态这个可能有两种情况： 卡在了查询状态，也就是mysql正在收集数据； 卡在了发送数据给客户端状态，这个可能是因为查询的数据量太大，堵塞了网络IO 分析&amp;解决这个sql出现问题的原因暂时还无解，因为 explain 之后的分析结果看，这个sql还是说得过去的，测试环境跑的也没问题，但是生产环境就有问题，有空向大拿请教一下。 解决思路是使用子查询，尽可能的在子查询中缩小数据查询范围，但是优化后的sql explain之后看并没有改善太多，很奇怪： 以下是优化前和优化后的sql，请有经验的同学不吝赐教，感谢！ 12345678910111213141516171819202122232425SELECT fl.id AS `fileLogId`, fl.MD5 AS `smallPortrait`, fl.userId AS `userId`, p.id AS `portraitId`, fl.updateTime AS `updateTime`FROM `CU_Log`.`UP_FileLog` fl, `CU`.`UP_Portrait` p, `CU`.`UP_User` uWHERE fl.userId = p.userId AND fl.userId = u.userId AND u.gender = 2 AND fl.`userId` &gt; 10000000 AND fl.`userId` &lt; 80000000 AND fl.`MD5` &lt;&gt; '94F46FECE6D8611C' AND fl.fileType = 2 AND fl.actionType = 1 AND fl.MD5 = p.smallPortrait AND p.`order` = (SELECT MIN(`order`) FROM `CU`.`UP_Portrait` WHERE userId = p.userId) AND fl.`MD5` NOT IN(SELECT `md5` FROM `CU`.`CU_BlackFile`) AND fl.`id` NOT IN(SELECT `fileLogId` FROM `CustomerSystem`.`CheckAvatarLog`) AND p.id NOT IN(SELECT `portraitId` FROM `CustomerSystem`.`CheckAvatarLog`)GROUP BY p.idORDER BY `updateTime` DESCLIMIT 8; 123456789101112131415161718192021222324252627282930SELECT f.fileLogId AS `fileLogId`, f.MD5 AS `smallPortrait`, u.userId AS `userId`, f.portraitId AS `portraitId`, f.updateTime AS `updateTime`FROM (SELECT fl.`id` AS `fileLogId`, fl.`MD5`, up.id AS portraitId, fl.`updateTime`, fl.`userId` FROM `CU_Log`.`UP_FileLog` fl, `CU`.`UP_Portrait` up WHERE fl.`userId` &gt; 10000000 AND fl.`userId` &lt; 80000000 AND fl.`actionType` = 1 AND fl.`fileType` = 2 AND fl.`MD5` &lt;&gt; '94F46FECE6D8611C' AND fl.`userId` IN(SELECT u.`userId` FROM `CU`.`UP_User` u WHERE u.`userId` = fl.`userId` AND u.`gender` = 2) AND fl.`MD5` NOT IN(SELECT bf.`md5` FROM `CU`.`CU_BlackFile` bf WHERE bf.`md5` = fl.`MD5`) AND fl.userId = up.userId AND fl.MD5 = up.smallPortrait AND up.`order` = (SELECT MIN(`order`) FROM `CU`.`UP_Portrait` WHERE userId = up.userId) GROUP BY fl.`userId` HAVING `fileLogId` NOT IN(SELECT cl.`fileLogId` FROM `CustomerSystem`.`CheckAvatarLog` cl WHERE cl.`fileLogId` = `fileLogId`) ORDER BY `updateTime` DESC LIMIT 8 ) f, `CU`.`UP_User` uWHERE f.`userId` = u.`userId`]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
</search>